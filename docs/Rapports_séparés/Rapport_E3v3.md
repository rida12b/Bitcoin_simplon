# Rapport d'Ã‰preuve E3 : IntÃ©gration, DÃ©ploiement et OpÃ©rations du Service d'IA (C9-C13)

Projet : **Bitcoin Analyzer**
Candidat : Rida Boualam
Date : Juillet 2025
Certification VisÃ©e : **RNCP37827 - DÃ©veloppeur en Intelligence Artificielle**

# Table des MatiÃ¨res

1.  Introduction : Du ModÃ¨le Fonctionnel au Service de Production (CompÃ©tences C9-C13)
    1.1. Contexte de l'Ã‰preuve : La Transition vers le RÃ©el
    1.2. L'Approche MLOps : Une Philosophie de FiabilitÃ© et d'Automatisation
2.  CompÃ©tence C9 : Exposer le ModÃ¨le d'IA via une API Robuste et Ã‰volutive
    2.1. Le RÃ´le StratÃ©gique de l'Endpoint `/price-analysis`
    2.2. L'Art du "Prompt Engineering" : Transformer l'IA en Expert MÃ©tier
        2.2.1. Ã‰tape 1 : Assigner un RÃ´le (Persona)
        2.2.2. Ã‰tape 2 : Fournir un Contexte Factuel
        2.2.3. Ã‰tape 3 : Poser une Question CiblÃ©e
        2.2.4. Ã‰tape 4 : Contraindre le Format de Sortie
    2.3. Analyse de l'ImplÃ©mentation dans le Code
    2.4. Preuve de l'EfficacitÃ© : Du Prompt Ã  l'Interface Utilisateur
    2.5. Fiabilisation de la Couche de DonnÃ©es : La Migration StratÃ©gique vers PostgreSQL
3.  CompÃ©tence C10 : IntÃ©grer l'API dans une Application en Conditions de Production
    3.1. Architecture de DÃ©ploiement : De localhost au Cloud
    3.2. Le RÃ´le de Gunicorn et Systemd : PÃ©renniser le Service
    3.3. Preuve de l'IntÃ©gration : L'Appel API en Environnement DÃ©ployÃ©
4.  CompÃ©tence C11 : Monitorer un Service d'IA DistribuÃ© et ses DÃ©pendances
    4.1. L'Importance Cruciale de la Journalisation (Logging)
    4.2. StratÃ©gie de Journalisation Multi-Couches pour une TraÃ§abilitÃ© ComplÃ¨te
    4.3. Preuve de Monitoring : Analyse d'un Log de Production
    4.4. Visualisation des Logs en Temps RÃ©el : Un Outil de DÃ©bogage Quotidien
    4.5. Surveillance des Processus AutomatisÃ©s : Le Cas du Job Cron
5.  CompÃ©tence C12 : Garantir la QualitÃ© par une StratÃ©gie de Tests AutomatisÃ©s SophistiquÃ©e
    5.1. Le ProblÃ¨me : Tester des DÃ©pendances Externes, CoÃ»teuses et ImprÃ©visibles
    5.2. Solution Niveau 1 : Isoler l'IA avec le Mocking
    5.3. Solution Niveau 2 : Isoler la Base de DonnÃ©es avec l'Injection de DÃ©pendances
    5.4. L'Impact StratÃ©gique : Une Suite de Tests Rapide et Fiable
    5.5. ExÃ©cution et Validation de la Suite de Tests
6.  CompÃ©tence C13 : De l'IntÃ©gration Continue (CI) au DÃ©ploiement Continu (CD)
    6.1. La Philosophie : Le "ZÃ©ro-Contact" pour des DÃ©ploiements SÃ©curisÃ©s
    6.2. Anatomie de mon Pipeline CI/CD sur GitHub Actions
        6.2.1. Job 1 : test - Le Gardien de la QualitÃ©
        6.2.2. Job 2 : package - La CrÃ©ation de l'Artefact
        6.2.3. Job 3 : deploy - La Mise en Production AutomatisÃ©e
    6.3. Le Script de DÃ©ploiement : L'Orchestrateur sur le Serveur
    6.4. Preuve de Validation : Analyse d'une ExÃ©cution RÃ©ussie
7.  Conclusion de l'Ã‰preuve E3 : La MaÃ®trise du Cycle de Vie MLOps
8.  Annexes
    Annexe A : Code Source Complet du Test du Module d'IA (test_llm_analyzer.py)
    Annexe B : Code Source Complet du Test d'API avec Injection de DÃ©pendances (test_api.py)
    Annexe C : Code Source Complet du Workflow CI/CD (ci.yml)
    Annexe D : Code Source Complet du Script de DÃ©ploiement (deploy.sh)

# 1. Introduction : Du ModÃ¨le Fonctionnel au Service de Production (CompÃ©tences C9-C13)

## 1.1. Contexte de l'Ã‰preuve : La Transition vers le RÃ©el

Les Ã©preuves prÃ©cÃ©dentes (E1, E2) ont permis de construire les fondations techniques du projet "**Bitcoin Analyzer**". Nous disposons d'un pipeline de donnÃ©es qui collecte et stocke l'information, et nous avons sÃ©lectionnÃ© un modÃ¨le d'Intelligence Artificielle performant (**Google Gemini**) sur la base d'un benchmark rigoureux. Cependant, un modÃ¨le fonctionnel en local n'est que la premiÃ¨re Ã©tape.

L'objectif de cette Ã©preuve E3 est de franchir le fossÃ© qui sÃ©pare un prototype d'un service de production. Il s'agit de transformer le modÃ¨le d'IA en un composant logiciel concret, robuste, accessible, et surtout, fiable dans un environnement rÃ©el. Avoir un bon modÃ¨le ne sert Ã  rien s'il n'est pas accessible de maniÃ¨re sÃ©curisÃ©e, si son fonctionnement n'est pas surveillÃ©, ou si chaque mise Ã  jour risque de provoquer une rÃ©gression.

## 1.2. L'Approche MLOps : Une Philosophie de FiabilitÃ© et d'Automatisation

Cette Ã©preuve couvre le cÅ“ur des pratiques **MLOps** (Machine Learning Operations). La philosophie **MLOps** consiste Ã  appliquer Ã  l'Intelligence Artificielle la mÃªme rigueur, les mÃªmes processus et les mÃªmes outils que ceux qui ont fait le succÃ¨s du dÃ©veloppement logiciel moderne (DevOps). L'objectif est de crÃ©er un cycle de vie complet et automatisÃ© qui garantit la qualitÃ© et la maintenabilitÃ©.

Pour ce projet, j'ai mis en place ce cycle de vie complet :

*   Exposer le modÃ¨le via une API de qualitÃ© professionnelle (C9).
*   IntÃ©grer cette API dans une application cliente dans un environnement de production dÃ©ployÃ© (C10).
*   Monitorer son fonctionnement pour comprendre son comportement et dÃ©tecter les problÃ¨mes (C11).
*   Tester son comportement de maniÃ¨re automatisÃ©e et isolÃ©e pour garantir la non-rÃ©gression (C12).
*   Valider et dÃ©ployer automatiquement chaque modification via une chaÃ®ne d'intÃ©gration et de dÃ©ploiement continus (**CI/CD**) (C13).

Ce rapport dÃ©taille la mise en Å“uvre de chacune de ces Ã©tapes, dÃ©montrant la capacitÃ© Ã  gÃ©rer un service d'IA de sa conception Ã  son exploitation en production.

# 2. CompÃ©tence C9 : Exposer le ModÃ¨le d'IA via une API Robuste et Ã‰volutive

## 2.1. Le RÃ´le StratÃ©gique de l'Endpoint `/price-analysis`

Pour que le modÃ¨le Gemini puisse Ãªtre utilisÃ© par d'autres applications, je l'ai exposÃ© via un endpoint spÃ©cifique dans mon API **FastAPI** : `/price-analysis`. Le rÃ´le de cet endpoint est bien plus qu'une simple passerelle. Il agit comme une couche d'abstraction intelligente qui transforme une technologie brute (le LLM) en un service mÃ©tier spÃ©cialisÃ© (un analyste financier pour dÃ©butants). Il prÃ©pare soigneusement la question pour obtenir la meilleure rÃ©ponse possible, garantissant ainsi la pertinence et la cohÃ©rence du service.

## 2.2. L'Art du "Prompt Engineering" : Transformer l'IA en Expert MÃ©tier

La qualitÃ© de la rÃ©ponse d'un LLM dÃ©pend Ã  90% de la qualitÃ© du prompt qu'on lui envoie. J'ai donc appliquÃ© une stratÃ©gie de "**prompt engineering**" directement dans mon code, en suivant 4 principes fondamentaux.

### 2.2.1. Ã‰tape 1 : Assigner un RÃ´le (Persona)

La premiÃ¨re ligne du prompt est : `Tu es un analyste financier pour un dÃ©butant.` Cette instruction est cruciale car elle active un contexte spÃ©cifique dans le modÃ¨le. Elle influence le ton (pÃ©dagogique et simple), le vocabulaire (Ã©viter le jargon technique) et le niveau de technicitÃ© de sa rÃ©ponse. C'est la diffÃ©rence entre une rÃ©ponse brute et une analyse vÃ©ritablement utile pour le persona cible "Alex".

### 2.2.2. Ã‰tape 2 : Fournir un Contexte Factuel

Je ne demande jamais Ã  l'IA de deviner ou d'utiliser des connaissances potentiellement obsolÃ¨tes. Je rÃ©cupÃ¨re l'historique rÃ©cent des prix depuis ma base de donnÃ©es **PostgreSQL** et je le formate en un texte simple et lisible. En fournissant ce contexte factuel, je m'assure que l'analyse est basÃ©e sur les donnÃ©es les plus fraÃ®ches et les plus pertinentes dont je dispose.

### 2.2.3. Ã‰tape 3 : Poser une Question CiblÃ©e

Le prompt ne se contente pas de demander "qu'en penses-tu ?". Il pose une question prÃ©cise et actionnable : `quelle est la tendance gÃ©nÃ©rale (haussiÃ¨re, baissiÃ¨re, ou stable) ?`. Je demande Ã©galement explicitement de mentionner si le marchÃ© semble volatil ou non. Une question prÃ©cise amÃ¨ne une rÃ©ponse prÃ©cise et utile.

### 2.2.4. Ã‰tape 4 : Contraindre le Format de Sortie

Pour que la rÃ©ponse soit directement utilisable dans l'interface utilisateur sans traitement complexe, je lui impose une contrainte de format : `RÃ©ponds en 2 phrases maximum`. Cette contrainte garantit une analyse concise, facile Ã  lire, qui s'intÃ¨gre parfaitement dans une carte du tableau de bord sans avoir Ã  Ãªtre tronquÃ©e ou reformatÃ©e.

## 2.3. Analyse de l'ImplÃ©mentation dans le Code

Voici comment cette stratÃ©gie est implÃ©mentÃ©e dans la fonction `price_analysis` de mon API. Chaque partie du prompt a un rÃ´le bien dÃ©fini pour guider le modÃ¨le.

Extrait de `api/app.py` : la logique de l'endpoint d'analyse IA.

```python
@app.get("/price-analysis", summary="Obtenir une analyse IA de la tendance des prix")
def price_analysis(limit: int = 24, conn=Depends(get_db_connection)):
    logging.info(f"RequÃªte reÃ§ue pour l'analyse de prix (limite={limit}).")
    try:
        # 1. RÃ©cupÃ©ration des donnÃ©es depuis la BDD de production (PostgreSQL)
        with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cursor:
            cursor.execute("SELECT timestamp, close FROM bitcoin_prices ORDER BY timestamp DESC LIMIT %s", (limit,))
            rows = cursor.fetchall()

        if not rows:
            raise HTTPException(status_code=404, detail="Pas assez de donnÃ©es pour l'analyse")

        # 2. Formatage des donnÃ©es pour le prompt
        formatted_history = "\n".join(
            [f"Date (timestamp {row['timestamp']}): Prix de clÃ´ture = {row['close']}$" for row in rows]
        )

        # 3. Construction du prompt structurÃ©
        prompt = (
            "Tu es un analyste financier pour un dÃ©butant. "  # Le RÃ´le (Persona)
            "BasÃ© sur l'historique de prix du Bitcoin suivant, quelle est la tendance gÃ©nÃ©rale (haussiÃ¨re, baissiÃ¨re, ou stable) ? "  # La Question
            "RÃ©ponds en 2 phrases maximum, en mentionnant si le marchÃ© semble volatil ou non.\n\n"  # Les Contraintes
            f"DonnÃ©es:\n{formatted_history}"  # Les DonnÃ©es
        )

        # 4. Appel au service d'analyse IA
        analysis_result = analyze_text(prompt)

        return {"analysis": analysis_result}

    except Exception as e:
        raise HTTPException(status_code=500, detail="Erreur interne du serveur lors de l'analyse IA")
```

## 2.4. Preuve de l'EfficacitÃ© : Du Prompt Ã  l'Interface Utilisateur

Pour valider l'efficacitÃ© de cette stratÃ©gie, la capture d'Ã©cran ci-dessous montre le rÃ©sultat final tel qu'affichÃ© Ã  l'utilisateur. On y voit une analyse concise et en langage simple, directement issue du prompt structurÃ© envoyÃ© Ã  l'API. C'est la preuve que la technique de prompt engineering a fonctionnÃ© comme prÃ©vu.

![alt text](../images/screen_result_analyse_ia.png)

Figure 1 : RÃ©sultat de l'analyse gÃ©nÃ©rÃ©e par l'IA, affichÃ©e sur le tableau de bord final.

## 2.5. Fiabilisation de la Couche de DonnÃ©es : La Migration StratÃ©gique vers PostgreSQL

Initialement, l'API fonctionnait avec **SQLite**. Cependant, pour une mise en production, cette solution prÃ©sentait des risques majeurs (perte de donnÃ©es sur les redÃ©ploiements, mauvaise gestion de la concurrence). La migration de la base de donnÃ©es vers **PostgreSQL**, hÃ©bergÃ©e sur un service managÃ©, a Ã©tÃ© une Ã©tape fondamentale pour fiabiliser l'endpoint d'analyse (C9).

Cette migration garantit que les donnÃ©es fournies au prompt de l'IA sont persistantes, fiables et cohÃ©rentes, ce qui est une condition sine qua non pour un service d'analyse de qualitÃ© professionnelle.

# 3. CompÃ©tence C10 : IntÃ©grer l'API dans une Application en Conditions de Production

## 3.1. Architecture de DÃ©ploiement : De localhost au Cloud

L'intÃ©gration de l'API ne se fait plus dans un environnement de dÃ©veloppement local, mais dans un vÃ©ritable environnement de production sur un VPS (Virtual Private Server) **DigitalOcean**. L'architecture dÃ©ployÃ©e est la suivante :

*   Backend (API **FastAPI**) : Est exÃ©cutÃ© par le serveur d'application **Gunicorn**, un standard de production bien plus robuste que le serveur de dÃ©veloppement d'Uvicorn.
*   Frontend (Application **Django**) : Est Ã©galement exÃ©cutÃ© par sa propre instance de **Gunicorn**.
*   Orchestration des Services : Les deux processus **Gunicorn** sont gÃ©rÃ©s comme des services Linux par **Systemd**. Cela garantit qu'ils sont lancÃ©s automatiquement au dÃ©marrage du serveur et redÃ©marrÃ©s en cas de crash, assurant ainsi une haute disponibilitÃ©.

## 3.2. Le RÃ´le de Gunicorn et Systemd : PÃ©renniser le Service

Utiliser **Gunicorn** et **Systemd** est une dÃ©monstration de compÃ©tence en administration systÃ¨me et en dÃ©ploiement.

*   **Gunicorn** gÃ¨re plusieurs processus "workers" pour traiter les requÃªtes en parallÃ¨le, ce qui est essentiel pour la performance.
*   **Systemd** transforme une simple commande en un service robuste, gÃ©rÃ© par le systÃ¨me d'exploitation.

## 3.3. Preuve de l'IntÃ©gration : L'Appel API en Environnement DÃ©ployÃ©

La compÃ©tence C10 est validÃ©e par la capacitÃ© du frontend **Django** Ã  consommer l'API de l'IA dans cet environnement dÃ©ployÃ©. Le code de la vue `viewer/views.py` utilise dÃ©sormais une variable d'environnement pour connaÃ®tre l'adresse de l'API, ce qui est une bonne pratique (*Twelve-Factor App*).

Extrait de `viewer/views.py` : la consommation de l'API en production.

```python
# Fichier: viewer/views.py
import requests
import os

# L'URL est configurable, ce qui permet de pointer vers le service Gunicorn de l'API
API_BASE_URL = os.environ.get('API_BASE_URL', 'http://127.0.0.1:8001')

def news_list(request):
    context = {}
    try:
        # Cet appel est une vÃ©ritable requÃªte rÃ©seau vers le service API sur le mÃªme serveur
        analysis_url = f"{API_BASE_URL}/price-analysis"
        analysis_response = requests.get(analysis_url, timeout=15)
        analysis_response.raise_for_status()
        context['price_analysis'] = analysis_response.json().get('analysis', "Format d'analyse inattendu.")

    except requests.exceptions.RequestException as e:
        # Cette gestion d'erreur est encore plus cruciale en production
        context['error_message'] = "Le service d'analyse est actuellement indisponible."

    return render(request, 'viewer/news_list.html', context)
```

Cet appel n'est plus une simple communication locale, mais une vÃ©ritable interaction inter-services dans un environnement de production.

# 4. CompÃ©tence C11 : Monitorer un Service d'IA DistribuÃ© et ses DÃ©pendances

## 4.1. L'Importance Cruciale de la Journalisation (Logging)

En production, on ne peut pas se contenter de "espÃ©rer que Ã§a marche". Il faut pouvoir suivre ce qu'il se passe, diagnostiquer les erreurs et surveiller les performances. Pour cela, j'ai intÃ©grÃ© le module `logging` de Python de maniÃ¨re systÃ©matique.

## 4.2. StratÃ©gie de Journalisation Multi-Couches pour une TraÃ§abilitÃ© ComplÃ¨te

Pour obtenir une visibilitÃ© complÃ¨te sur le systÃ¨me, une stratÃ©gie de journalisation a Ã©tÃ© implÃ©mentÃ©e Ã  chaque niveau de l'architecture dÃ©couplÃ©e :

*   Frontend (**Django**) : Le fichier `viewer/views.py` utilise le logger pour tracer les Ã©vÃ©nements liÃ©s Ã  l'utilisateur : la rÃ©ception d'une requÃªte HTTP, le dÃ©but de chaque appel sortant vers l'API **FastAPI**, et les succÃ¨s ou Ã©checs de ces appels.
*   Backend (**FastAPI**) : Le fichier `api/app.py` logue toutes les actions internes du service : la rÃ©ception d'une requÃªte, les interactions avec la base de donnÃ©es **PostgreSQL**, et surtout, le dÃ©but et la fin de l'appel critique vers le service externe d'IA (**Google Gemini**).

Cette approche permet une traÃ§abilitÃ© de bout en bout : il est possible de suivre le parcours complet d'une requÃªte, depuis le clic de l'utilisateur jusqu'Ã  la rÃ©ponse de l'IA, en corrÃ©lant les logs des deux services.

## 4.3. Preuve de Monitoring : Analyse d'un Log de Production

Le bloc de texte ci-dessous, extrait de `docs/exemples_de_logs.txt`, montre la trace exacte d'un appel rÃ©ussi Ã  l'API d'analyse. C'est une mine d'informations pour un ingÃ©nieur :

*   Timestamp (`2025-07-09 10:23:39,098`) : Permet de corrÃ©ler les Ã©vÃ©nements.
*   Niveau de Log (`INFO`) : Indique un Ã©vÃ©nement normal.
*   Message : DÃ©crit prÃ©cisÃ©ment l'action.

En comparant le timestamp du log "Appel au service d'analyse IA" et "Analyse IA reÃ§ue avec succÃ¨s", je peux calculer que l'appel Ã  l'API Gemini et son analyse ont pris environ 7 secondes, une mÃ©trique de performance essentielle.

Extrait de `docs/exemples_de_logs.txt` :

```text
2025-07-09 10:23:39,098 - INFO - API - RequÃªte reÃ§ue pour l'analyse de prix (limite=24).
2025-07-09 10:23:39,103 - INFO - API - Appel au service d'analyse IA (Gemini)...
2025-07-09 10:23:46,068 - INFO - API - Analyse IA reÃ§ue avec succÃ¨s.
INFO: 127.0.0.1:61446 - "GET /price-analysis HTTP/1.1" 200 OK
```

## 4.4. Visualisation des Logs en Temps RÃ©el : Un Outil de DÃ©bogage Quotidien

En complÃ©ment de l'analyse des fichiers de logs, la sortie du serveur **FastAPI** en temps rÃ©el est un outil de dÃ©buggage indispensable. La capture ci-dessous montre les logs gÃ©nÃ©rÃ©s par le serveur lors d'un appel Ã  l'endpoint `/price-analysis`.

![alt text](../images/log_terminal.png)

Figure 2 : Logs du serveur FastAPI affichÃ©s en temps rÃ©el lors d'un appel Ã  l'endpoint d'analyse.

## 4.5. Surveillance des Processus AutomatisÃ©s : Le Cas du Job Cron

Le monitoring ne s'arrÃªte pas aux services interactifs. La chaÃ®ne de collecte de donnÃ©es, qui est le cÅ“ur du pipeline de donnÃ©es, est automatisÃ©e en production via un job Cron. Pour surveiller son bon fonctionnement, sa sortie standard et ses erreurs sont redirigÃ©es vers un fichier de log dÃ©diÃ©.

Extrait de la configuration crontab sur le serveur de production :

```bash
# ExÃ©cute le script de collecte toutes les heures et logue la sortie
0 * * * * cd /root/Bitcoin_simplon/ && /root/Bitcoin_simplon/venv/bin/python run_scripts.sh >> /root/Bitcoin_simplon/cron.log 2>&1
```

La consultation rÃ©guliÃ¨re du fichier `cron.log` est une preuve directe de la surveillance (C11) d'un processus automatisÃ© essentiel au bon fonctionnement du service d'IA. C'est une pratique **MLOps** fondamentale.

# 5. CompÃ©tence C12 : Garantir la QualitÃ© par une StratÃ©gie de Tests AutomatisÃ©s SophistiquÃ©e

## 5.1. Le ProblÃ¨me : Tester des DÃ©pendances Externes, CoÃ»teuses et ImprÃ©visibles

Tester un composant qui dÃ©pend de services externes (une API d'IA payante, une base de donnÃ©es de production) pose plusieurs problÃ¨mes :

*   C'est lent : Les appels rÃ©seau ralentissent considÃ©rablement la suite de tests.
*   C'est cher : Chaque test de l'IA effectuerait un vrai appel facturÃ©.
*   Ce n'est pas fiable : Un test peut Ã©chouer Ã  cause d'un problÃ¨me rÃ©seau ou d'une indisponibilitÃ© du service externe, et non Ã  cause d'un bug dans mon code.
*   C'est dangereux : Les tests ne doivent jamais interagir avec la base de donnÃ©es de production.

Pour surmonter ces dÃ©fis, j'ai mis en place une stratÃ©gie de test Ã  deux niveaux.

## 5.2. Solution Niveau 1 : Isoler l'IA avec le Mocking

La solution pour tester la logique d'appel Ã  l'IA est le mocking. Dans mon fichier de test `tests/test_llm_analyzer.py`, j'utilise le dÃ©corateur `@patch` de la bibliothÃ¨que `unittest.mock` pour remplacer l'appel rÃ©el Ã  l'API Gemini par un "imitateur" (un mock) qui renvoie une rÃ©ponse prÃ©visible et contrÃ´lÃ©e.

Cette approche me permet de valider que ma fonction construit le bon prompt et traite correctement la rÃ©ponse simulÃ©e, le tout en quelques millisecondes, gratuitement et de maniÃ¨re 100% fiable.

![alt text](../images/test_mocking.png)

Figure 3 : Code du test unitaire pour le module d'analyse, utilisant le mocking pour isoler l'appel Ã  l'API externe.

## 5.3. Solution Niveau 2 : Isoler la Base de DonnÃ©es avec l'Injection de DÃ©pendances

Lors de la migration vers **PostgreSQL**, un nouveau dÃ©fi est apparu : comment tester l'API sans qu'elle ne se connecte Ã  la base de donnÃ©es de production ? La solution est une technique avancÃ©e offerte par **FastAPI** : l'injection de dÃ©pendances.

Dans le fichier de test `tests/test_api.py`, je surcharge la fonction qui fournit la connexion Ã  la base de donnÃ©es. Pendant l'exÃ©cution des tests, au lieu d'appeler la fonction qui se connecte Ã  **PostgreSQL**, **FastAPI** appellera une fonction de substitution qui se connecte Ã  une base de donnÃ©es **SQLite** de test, temporaire et isolÃ©e.

Extrait de `tests/test_api.py` : Surcharge de la dÃ©pendance de la base de donnÃ©es.

```python
# Fichier: tests/test_api.py
import sys
import os
import sqlite3
from fastapi.testclient import TestClient

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from api.app import app, get_db_connection

# Chemin vers notre base de donnÃ©es de test
TEST_DB_PATH = os.path.join(os.path.dirname(__file__), 'test_database.db')

# Fonction de substitution pour la connexion Ã  la base de donnÃ©es
def override_get_db_connection():
    conn = sqlite3.connect(TEST_DB_PATH)
    try:
        yield conn
    finally:
        conn.close()

# Surcharge de la dÃ©pendance pour les tests
app.dependency_overrides[get_db_connection] = override_get_db_connection

client = TestClient(app)

def test_price_history_uses_test_db():
    # Cet appel se fera sur la BDD de test SQLite, pas sur PostgreSQL !
    response = client.get("/price-history")
    assert response.status_code == 200
```

## 5.4. L'Impact StratÃ©gique : Une Suite de Tests Rapide et Fiable

Cette stratÃ©gie de test Ã  deux niveaux est une dÃ©monstration de compÃ©tence professionnelle. Elle me permet de maintenir une suite de tests extrÃªmement rapide, fiable et totalement isolÃ©e, alors mÃªme que l'application en production utilise une architecture plus complexe (**PostgreSQL**). C'est la garantie que je peux faire Ã©voluer mon application en toute confiance, en validant chaque changement sans risque et sans ralentir le cycle de dÃ©veloppement.

## 5.5. ExÃ©cution et Validation de la Suite de Tests

La preuve finale est l'exÃ©cution rÃ©ussie de la suite de tests avec `pytest`, qui valide que tous les composants fonctionnent comme attendu, y compris ceux qui interagissent avec les mocks.

![alt text](../images/run_pytest.png)

Figure 4 : RÃ©sultat de l'exÃ©cution de la suite de tests pytest, montrant la validation rÃ©ussie des composants.

# 6. CompÃ©tence C13 : De l'IntÃ©gration Continue (CI) au DÃ©ploiement Continu (CD)

## 6.1. La Philosophie : Le "ZÃ©ro-Contact" pour des DÃ©ploiements SÃ©curisÃ©s

L'objectif ultime d'un pipeline **MLOps** est de crÃ©er une chaÃ®ne entiÃ¨rement automatisÃ©e qui va de la modification du code Ã  sa mise en production, sans intervention humaine. C'est ce qu'on appelle le DÃ©ploiement Continu (**CD**). Cette automatisation est un filet de sÃ©curitÃ© qui garantit la qualitÃ© et la non-rÃ©gression du code tout en minimisant les risques d'erreur humaine.

## 6.2. Anatomie de mon Pipeline CI/CD sur GitHub Actions

Mon pipeline, dÃ©fini dans le fichier `.github/workflows/ci.yml`, a Ã©tÃ© transformÃ© d'une simple chaÃ®ne de CI (qui ne fait que tester) en une chaÃ®ne de **CI/CD** complÃ¨te. Il se dÃ©roule en trois Ã©tapes (ou "jobs") sÃ©quentielles :

### 6.2.1. Job 1 : test - Le Gardien de la QualitÃ©

Cette Ã©tape est le cÅ“ur de l'IntÃ©gration Continue (**CI**). Elle recrÃ©e un environnement propre Ã  partir de zÃ©ro, installe les dÃ©pendances, prÃ©pare la base de donnÃ©es de test, et exÃ©cute la suite de tests `pytest`. Si la moindre erreur est dÃ©tectÃ©e, le pipeline s'arrÃªte immÃ©diatement, empÃªchant le code dÃ©fectueux d'aller plus loin.

### 6.2.2. Job 2 : package - La CrÃ©ation de l'Artefact

Si les tests rÃ©ussissent, cette Ã©tape construit l'image Docker de l'API. Le rÃ©sultat de cette Ã©tape est un artefact de dÃ©ploiement : un conteneur standard, portable et autonome qui embarque l'application et toutes ses dÃ©pendances.

### 6.2.3. Job 3 : deploy - La Mise en Production AutomatisÃ©e

C'est cette Ã©tape qui transforme la CI en CD. Si l'image Docker est construite avec succÃ¨s, le workflow se connecte en SSH au VPS de production. Cette connexion est hautement sÃ©curisÃ©e grÃ¢ce Ã  l'utilisation des GitHub Secrets, qui stockent la clÃ© privÃ©e SSH et les identifiants du serveur sans jamais les exposer dans le code. Une fois connectÃ©, il exÃ©cute le script de dÃ©ploiement.

![alt text](../images/ci.yml.png)

Figure 5 : Code source du workflow GitHub Actions dÃ©fini dans `ci.yml`, montrant les jobs de test, de packaging et de dÃ©ploiement.

## 6.3. Le Script de DÃ©ploiement : L'Orchestrateur sur le Serveur

Le script `deploy.sh` prÃ©sent sur le serveur est appelÃ© par le pipeline et orchestre les derniÃ¨res Ã©tapes :

1.  `git reset --hard origin/main` : Force la synchronisation du code avec la derniÃ¨re version validÃ©e.
2.  `pip install -r requirements.txt` : Met Ã  jour les dÃ©pendances Python.
3.  `sudo systemctl restart gunicorn_api` et `sudo systemctl restart gunicorn_django` : RedÃ©marre proprement les services Gunicorn pour appliquer les changements sans interruption de service majeure.

## 6.4. Preuve de Validation : Analyse d'une ExÃ©cution RÃ©ussie

Chaque push sur la branche `main` dÃ©clenche ce pipeline. Une exÃ©cution rÃ©ussie, visible dans l'onglet "Actions" de GitHub, est la preuve d'un processus **MLOps** fonctionnel. On y voit la coche verte Ã  chaque Ã©tape (test, package, deploy), matÃ©rialisant le cheminement automatisÃ© du code, de mon poste de dÃ©veloppeur jusqu'au serveur de production.

![alt text](../images/exemple_cicd.png)

Figure 6 : ExÃ©cution rÃ©ussie du workflow d'intÃ©gration et dÃ©ploiement continus sur GitHub Actions, validant chaque Ã©tape du processus.

# 7. Conclusion de l'Ã‰preuve E3 : La MaÃ®trise du Cycle de Vie MLOps

Ã€ ce stade, le service d'IA du projet "**Bitcoin Analyzer**" est bien plus qu'un simple script ou un composant logiciel robuste. C'est un service dÃ©ployÃ© en production, dont le fonctionnement est surveillÃ© en temps rÃ©el, dont la qualitÃ© est validÃ©e par une stratÃ©gie de tests automatisÃ©s sophistiquÃ©e, et dont les mises Ã  jour sont entiÃ¨rement automatisÃ©es par un pipeline de **CI/CD** professionnel.

Cette approche **MLOps** de bout en bout, qui va de l'intÃ©gration du modÃ¨le Ã  son exploitation opÃ©rationnelle, garantit non seulement la fiabilitÃ© et la maintenabilitÃ© du cÅ“ur intelligent de l'application, mais aussi sa capacitÃ© Ã  Ã©voluer en toute sÃ©curitÃ©. J'ai dÃ©montrÃ© ma capacitÃ© Ã  gÃ©rer le cycle de vie complet d'un service d'IA, une compÃ©tence fondamentale pour tout dÃ©veloppeur en intelligence artificielle.

# 8. Annexes

## Annexe A : Code Source Complet du Test du Module d'IA (test_llm_analyzer.py)

```python
# Fichier: tests/test_llm_analyzer.py
import pytest
from unittest.mock import patch, MagicMock
from scripts.llm_analyzer import analyze_text

# Le dÃ©corateur @patch intercepte tous les appels Ã  'genai.GenerativeModel'
# dans le module 'scripts.llm_analyzer' et le remplace par un mock.
@patch('scripts.llm_analyzer.genai.GenerativeModel')
def test_analyze_text_with_mock(mock_generative_model):
    """
    VÃ©rifie que notre fonction `analyze_text` appelle bien le modÃ¨le simulÃ©
    et retourne le texte de la rÃ©ponse attendue.
    """
    # --- 1. PrÃ©paration (Arrange) ---
    # On dÃ©finit ce que la rÃ©ponse simulÃ©e (le "mock") doit retourner.
    fake_response_text = "Ceci est une analyse simulÃ©e rÃ©ussie."

    # On configure notre objet simulÃ© pour qu'il se comporte comme le vrai objet genai
    mock_model_instance = MagicMock()
    mock_model_instance.generate_content.return_value.text = fake_response_text
    mock_generative_model.return_value = mock_model_instance

    prompt_test = "Ceci est un prompt de test."

    # --- 2. Action (Act) ---
    # On appelle notre fonction. Le @patch va intercepter l'appel Ã  genai.
    result = analyze_text(prompt_test)

    # --- 3. VÃ©rification (Assert) ---
    # On vÃ©rifie que le modÃ¨le simulÃ© a bien Ã©tÃ© appelÃ© avec notre prompt.
    mock_model_instance.generate_content.assert_called_once_with(prompt_test)

    # On vÃ©rifie que le rÃ©sultat de notre fonction est bien celui qu'on a simulÃ©.
    assert result == fake_response_text
```

## Annexe B : Code Source Complet du Test d'API avec Injection de DÃ©pendances (test_api.py)

```python
# Fichier: tests/test_api.py
import sys
import os
import sqlite3
from fastapi.testclient import TestClient

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from api.app import app, get_db_connection

# Chemin vers notre base de donnÃ©es de test
TEST_DB_PATH = os.path.join(os.path.dirname(__file__), 'test_database.db')

# Fonction de substitution pour la connexion Ã  la base de donnÃ©es
def override_get_db_connection():
    conn = sqlite3.connect(TEST_DB_PATH)
    try:
        yield conn
    finally:
        conn.close()

# Surcharge de la dÃ©pendance pour les tests
app.dependency_overrides[get_db_connection] = override_get_db_connection

client = TestClient(app)

def test_get_latest_news():
    """Teste si l'API retourne bien la nouvelle de test depuis la BDD de test."""
    response = client.get("/latest-news")
    assert response.status_code == 200
    data = response.json()
    assert len(data) >= 1
    assert data[0]['title'] == "Titre de test"

def test_get_price_history():
    """Teste si l'API retourne bien l'historique de prix de test."""
    response = client.get("/price-history?limit=3")
    assert response.status_code == 200
    data = response.json()
    assert len(data) == 3
```

## Annexe C : Code Source Complet du Workflow CI/CD (ci.yml)

```yaml
# Fichier: .github/workflows/ci.yml
name: Python Application CI/CD

on:
  push:
    branches: [ "main" ]

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Prepare test database
        run: python tests/setup_test_db.py
      - name: Run tests with pytest
        run: pytest

  package:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: test
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build Docker image
        run: |
          docker build -t bitcoin-analyzer:latest .
          echo "Image Docker construite avec succÃ¨s."

  deploy:
    name: Deploy to DigitalOcean VPS
    runs-on: ubuntu-latest
    needs: [test, package]
    steps:
    - name: Deploy to VPS
      uses: appleboy/ssh-action@master
      with:
        host: ${{ secrets.VPS_HOST }}
        username: ${{ secrets.VPS_USERNAME }}
        key: ${{ secrets.VPS_SSH_PRIVATE_KEY }}
        script: |
          cd /root/Bitcoin_simplon
          ./deploy.sh
```

## Annexe D : Code Source Complet du Script de DÃ©ploiement (deploy.sh)

```bash
#!/bin/bash
# ArrÃªte le script si une commande Ã©choue
set -e

echo "ðŸš€ DÃ©marrage du dÃ©ploiement..."

# 1. Se positionner dans le rÃ©pertoire du projet
# (Note: Le workflow SSH le fait dÃ©jÃ , mais c'est une bonne pratique)
# cd /root/Bitcoin_simplon

# 2. Forcer la synchronisation avec la derniÃ¨re version de la branche main
echo "ðŸ”„ Synchronisation du code source avec GitHub..."
git fetch origin
git reset --hard origin/main

# 3. Mettre Ã  jour les dÃ©pendances Python
echo "ðŸ“¦ Installation/Mise Ã  jour des dÃ©pendances..."
/root/Bitcoin_simplon/venv/bin/pip install -r requirements.txt

# 4. RedÃ©marrer les services pour appliquer les changements
echo "ðŸ”„ RedÃ©marrage des services Gunicorn via Systemd..."
sudo systemctl restart gunicorn_api
sudo systemctl restart gunicorn_django
echo "âœ… Services redÃ©marrÃ©s avec succÃ¨s."

echo "ðŸŽ‰ DÃ©ploiement terminÃ© !"
```